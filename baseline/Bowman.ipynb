{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“HW2_wyc.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dKfdbsQDgJh",
        "colab_type": "code",
        "outputId": "ebd27ac6-ab62-428c-af22-041b1b71bdae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Dataset\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import time\n",
        "import dill\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU4j19JiV2kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "\n",
        "new_dir = []\n",
        "project_dir_path = './'\n",
        "data_dir_path = join(project_dir_path, 'data')\n",
        "model_dir_path = join(project_dir_path, 'model')\n",
        "new_dir.append(data_dir_path)\n",
        "new_dir.append(model_dir_path)\n",
        "snli_split_dir_path = join(data_dir_path, 'snli_split')\n",
        "snli_train_examples_path = join(snli_split_dir_path, 'train_examples')\n",
        "snli_dev_examples_path = join(snli_split_dir_path, 'dev_examples')\n",
        "snli_test_examples_path = join(snli_split_dir_path, 'test_examples')\n",
        "\n",
        "snli_split_path_lst = [snli_train_examples_path, snli_dev_examples_path, snli_test_examples_path]\n",
        "\n",
        "snli_text_vocab_path = join(snli_split_dir_path, 'text_vocab')\n",
        "snli_label_vocab_path = join(snli_split_dir_path, 'label_vocab')\n",
        "\n",
        "new_dir.append(snli_split_dir_path)\n",
        "\n",
        "for dir in new_dir:\n",
        "    if not os.path.exists(dir):\n",
        "        print('mkdir:', dir)\n",
        "        os.mkdir(dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T62lOGEWGhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLIDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return data.interleave_keys(\n",
        "            len(ex.premise), len(ex.hypothesis))\n",
        "\n",
        "\n",
        "class SNLI(object):\n",
        "    def __init__(self, batch_size=4, gpu=torch.device(torch.cuda.current_device())):\n",
        "        self.TEXT = data.Field(batch_first=True,\n",
        "                               include_lengths=True,\n",
        "                               tokenize=word_tokenize,\n",
        "                               lower=True)\n",
        "\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None)\n",
        "\n",
        "        # Split Dataset\n",
        "        if self.if_split_already():\n",
        "            print('Loading splited data set...')\n",
        "            fields = {'premise': self.TEXT, 'hypothesis': self.TEXT, 'label': self.LABEL}\n",
        "            self.train, self.dev, self.test = self.load_split_datasets(fields)\n",
        "        else:\n",
        "            print('No local data set detected, spliting...')\n",
        "            self.train, self.dev, self.test = datasets.SNLI.splits(self.TEXT, self.LABEL, root='data')\n",
        "            self.dump_examples(self.train, self.dev, self.test)\n",
        "\n",
        "\n",
        "        # Create Vocab\n",
        "        print('Building Vocab...')\n",
        "        # self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "        # self.LABEL.build_vocab(self.train)\n",
        "        if os.path.exists(snli_text_vocab_path) and os.path.exists(snli_label_vocab_path):\n",
        "            print('Loading local Vocab...')\n",
        "            with open(snli_text_vocab_path, 'rb')as f:\n",
        "                self.TEXT.vocab = dill.load(f)\n",
        "            with open(snli_label_vocab_path, 'rb')as f:\n",
        "                self.LABEL.vocab = dill.load(f)\n",
        "        else:\n",
        "            print('No local Vocab detected, building...')\n",
        "            self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "            self.LABEL.build_vocab(self.train)\n",
        "            with open(snli_text_vocab_path, 'wb')as f:\n",
        "                dill.dump(self.TEXT.vocab, f)\n",
        "            with open(snli_label_vocab_path, 'wb')as f:\n",
        "                dill.dump(self.LABEL.vocab, f)\n",
        "\n",
        "\n",
        "        # Generate batch iterator\n",
        "        print('Generating batch iter...')\n",
        "        self.train_iter, self.dev_iter, self.test_iter = \\\n",
        "            data.BucketIterator.splits((self.train, self.dev, self.test),\n",
        "                                       batch_size=batch_size,\n",
        "                                       device=gpu)\n",
        "\n",
        "    def if_split_already(self):\n",
        "        for path in snli_split_path_lst:\n",
        "            if not os.path.exists(path):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # Load dataset from local\n",
        "    def load_split_datasets(self, fields):\n",
        "        # Loading examples\n",
        "        with open(snli_train_examples_path, 'rb')as f:\n",
        "            train_examples = dill.load(f)\n",
        "        with open(snli_dev_examples_path, 'rb')as f:\n",
        "            dev_examples = dill.load(f)\n",
        "        with open(snli_test_examples_path, 'rb')as f:\n",
        "            test_examples = dill.load(f)\n",
        "\n",
        "        # Recover dataset\n",
        "        train = SNLIDataset(examples=train_examples, fields=fields)\n",
        "        dev = SNLIDataset(examples=dev_examples, fields=fields)\n",
        "        test = SNLIDataset(examples=test_examples, fields=fields)\n",
        "        return train, dev, test\n",
        "\n",
        "    # Save to local\n",
        "    def dump_examples(self, train, dev, test):\n",
        "        # Save examples\n",
        "        if not os.path.exists(snli_train_examples_path):\n",
        "            with open(snli_train_examples_path, 'wb')as f:\n",
        "                dill.dump(train.examples, f)\n",
        "        if not os.path.exists(snli_dev_examples_path):\n",
        "            with open(snli_dev_examples_path, 'wb')as f:\n",
        "                dill.dump(dev.examples, f)\n",
        "        if not os.path.exists(snli_test_examples_path):\n",
        "            with open(snli_test_examples_path, 'wb')as f:\n",
        "                dill.dump(test.examples, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C4zrNONWwSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1f84533c-d52c-43f9-b183-9e7fc6198e9e"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "snli = SNLI(batch_size=32, gpu=device)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading splited data set...\n",
            "Building Vocab...\n",
            "Loading local Vocab...\n",
            "Generating batch iter...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UHTtZ3-b1QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bowman(nn.Module):\n",
        "    def __init__(self, vocab, premise_emb=300, hypothesis_emb=300, premise_d=100, hypothesis_d=100, lstm_layers=1, dropout=0.1):\n",
        "        super(Bowman, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Premise_Enc = nn.LSTM(input_size=premise_emb, hidden_size=premise_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.Hypothesis_Enc = nn.LSTM(input_size=hypothesis_emb, hidden_size=hypothesis_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.out = nn.Linear(premise_d + hypothesis_d, 3)\n",
        "\n",
        "    def forward(self, premise_seq, hypothesis_seq):\n",
        "        premise_seq = self.embedding(premise_seq)\n",
        "        hypothesis_seq = self.embedding(hypothesis_seq)\n",
        "        premise_seq = self.dropout(premise_seq)\n",
        "        hypothesis_seq = self.dropout(hypothesis_seq)\n",
        "\n",
        "        premise_output, _  = self.Premise_Enc(premise_seq)\n",
        "        hypothesis_output, _  = self.Hypothesis_Enc(hypothesis_seq)\n",
        "        premise_output = torch.mean(premise_output, 1)\n",
        "        hypothesis_output = torch.mean(hypothesis_output, 1)\n",
        "        next_in = torch.cat((premise_output, hypothesis_output), 1)\n",
        "        #next_in = torch.cat((premise_output[ :, -1, :],hypothesis_output[ :, -1, :]), 1)\n",
        "        next_in = self.dropout(next_in)\n",
        "        tanh_out = self.tanh(self.tanh(self.tanh(next_in)))\n",
        "        output = self.out(tanh_out)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTYWSOXb6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_train(model, dataset, criterion, optimizer, epoch_num=5):\n",
        "    snli = dataset\n",
        "    record = open(\"result.txt\", \"w\")\n",
        "\n",
        "    for epoch in range(epoch_num):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        epoch_loss = 0.0\n",
        "        i = 0\n",
        "        for batch in snli.train_iter:\n",
        "            i += 1\n",
        "            #get data\n",
        "        \n",
        "            premise, _ = batch.premise\n",
        "            premise.to(device)\n",
        "            hypothesis, _ = batch.hypothesis\n",
        "            hypothesis.to(device)\n",
        "            label = batch.label\n",
        "            label.to(device)\n",
        "\n",
        "            # zeros the paramster gradients\n",
        "            optimizer.zero_grad()       # \n",
        "\n",
        "            # forward + backward + optimize\n",
        "            output = model(premise, hypothesis)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item() \n",
        "            epoch_loss = loss.item()\n",
        "            if i % 1000 == 999:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch, i + 1, running_loss / 1000))\n",
        "                running_loss = 0.0\n",
        "        print('epoch %d loss: %.3f\\n' % (epoch, epoch_loss))\n",
        "        record.write('epoch %d loss: %.3f\\n' % (epoch, epoch_loss))\n",
        "        torch.save(model, './model/bowman_%d.pt'% (epoch))\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(model, './model/bowman_final.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPnY8SIdxaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50799cd7-a5f9-450a-d21b-89d92f90ca88"
      },
      "source": [
        "model = Bowman(snli.TEXT.vocab)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n",
        "\n",
        "bowman_train(model, snli, criterion, optimizer, epoch_num=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0,  1000] loss: 1.087\n",
            "[0,  2000] loss: 1.041\n",
            "[0,  3000] loss: 0.987\n",
            "[0,  4000] loss: 0.946\n",
            "[0,  5000] loss: 0.927\n",
            "[0,  6000] loss: 0.920\n",
            "[0,  7000] loss: 0.905\n",
            "[0,  8000] loss: 0.895\n",
            "[0,  9000] loss: 0.892\n",
            "[0, 10000] loss: 0.889\n",
            "[0, 11000] loss: 0.884\n",
            "[0, 12000] loss: 0.876\n",
            "[0, 13000] loss: 0.861\n",
            "[0, 14000] loss: 0.864\n",
            "[0, 15000] loss: 0.865\n",
            "[0, 16000] loss: 0.856\n",
            "[0, 17000] loss: 0.846\n",
            "epoch 0 loss: 1.059\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Bowman. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 0.844\n",
            "[1,  2000] loss: 0.837\n",
            "[1,  3000] loss: 0.839\n",
            "[1,  4000] loss: 0.829\n",
            "[1,  5000] loss: 0.837\n",
            "[1,  6000] loss: 0.832\n",
            "[1,  7000] loss: 0.827\n",
            "[1,  8000] loss: 0.823\n",
            "[1,  9000] loss: 0.822\n",
            "[1, 10000] loss: 0.817\n",
            "[1, 11000] loss: 0.814\n",
            "[1, 12000] loss: 0.813\n",
            "[1, 13000] loss: 0.818\n",
            "[1, 14000] loss: 0.816\n",
            "[1, 15000] loss: 0.813\n",
            "[1, 16000] loss: 0.810\n",
            "[1, 17000] loss: 0.808\n",
            "epoch 1 loss: 0.817\n",
            "\n",
            "[2,  1000] loss: 0.796\n",
            "[2,  2000] loss: 0.809\n",
            "[2,  3000] loss: 0.800\n",
            "[2,  4000] loss: 0.799\n",
            "[2,  5000] loss: 0.803\n",
            "[2,  6000] loss: 0.801\n",
            "[2,  7000] loss: 0.799\n",
            "[2,  8000] loss: 0.793\n",
            "[2,  9000] loss: 0.797\n",
            "[2, 10000] loss: 0.792\n",
            "[2, 11000] loss: 0.795\n",
            "[2, 12000] loss: 0.787\n",
            "[2, 13000] loss: 0.791\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}