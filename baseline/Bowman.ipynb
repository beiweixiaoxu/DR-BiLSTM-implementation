{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bowman.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhBAy7RsMdDc",
        "colab_type": "text"
      },
      "source": [
        "# Bowman's Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut10idNFMQ71",
        "colab_type": "text"
      },
      "source": [
        "## Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dKfdbsQDgJh",
        "colab_type": "code",
        "outputId": "bdd7400b-fa0f-4364-fd1f-772fd194d414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Dataset\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import time\n",
        "import dill\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUcE6lFVMsnW",
        "colab_type": "text"
      },
      "source": [
        "## Make directories\n",
        "Make directories for saving data and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU4j19JiV2kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "\n",
        "# claim directories for saving data and model\n",
        "new_dir = ['./data', './model', './data/snli_split']\n",
        "\n",
        "# if directories not exist, make new directory\n",
        "for dir in new_dir:\n",
        "    if not os.path.exists(dir):\n",
        "        print('mkdir:', dir)\n",
        "        os.mkdir(dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY_HL4ngM839",
        "colab_type": "text"
      },
      "source": [
        "## SNLI corpus data preprocessing\n",
        "The class to preprocess SNLI corpus.\n",
        "\n",
        "For the first time on initial, the initial function will download and split SNLI corpus to train, dev and test sets, build vocab for the corpus by using Glove 840B 300d and store them to local files. And it will also generate batch iterator for the sets. This process may take more than 5 minutes on first time.\n",
        "\n",
        "After the first time, initial function will check existence of local files, if exists, it will load directly from local file and thus can save a lot of time (within 1 minute)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T62lOGEWGhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLIDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return data.interleave_keys(len(ex.premise), len(ex.hypothesis))\n",
        "\n",
        "# preprocess SNLI corpus to save time and give train, dev, test sets\n",
        "class SNLI(object):\n",
        "    def __init__(self, batch_size=4, gpu=torch.device('cuda')):\n",
        "        # set file name for train dev test sets\n",
        "        self.snli_split_path_lst = ['./data/snli_split/train', './data/snli_split/dev', './data/snli_split/test']\n",
        "\n",
        "        # set data field for text and label\n",
        "        self.TEXT = data.Field(batch_first=True, include_lengths=True, tokenize=word_tokenize, lower=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None)\n",
        "\n",
        "        # split corpus\n",
        "        if self.if_splited():\n",
        "            # if already splited, load local sets\n",
        "            fields = {'premise': self.TEXT, 'hypothesis': self.TEXT, 'label': self.LABEL}\n",
        "            self.train, self.dev, self.test = self.load_split_datasets(fields)\n",
        "        else:\n",
        "            # split corpus to train, dev, test sets and save them to local\n",
        "            self.train, self.dev, self.test = datasets.SNLI.splits(self.TEXT, self.LABEL, root='data')\n",
        "            self.save_splited_sets(self.train, self.dev, self.test)\n",
        "\n",
        "\n",
        "        # build vocab for corpus\n",
        "        if os.path.exists('./data/snli_split/text_vocab') and os.path.exists('./data/snli_split/label_vocab'):\n",
        "            # if local vocab exists, load local vocab into model\n",
        "            with open('./data/snli_split/text_vocab', 'rb')as f:\n",
        "                self.TEXT.vocab = dill.load(f)\n",
        "            with open('./data/snli_split/label_vocab', 'rb')as f:\n",
        "                self.LABEL.vocab = dill.load(f)\n",
        "        else:\n",
        "            # build vocab for corpus and save it to local\n",
        "            self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "            self.LABEL.build_vocab(self.train)\n",
        "            with open('./data/snli_split/text_vocab', 'wb')as f:\n",
        "                dill.dump(self.TEXT.vocab, f)\n",
        "            with open('./data/snli_split/label_vocab', 'wb')as f:\n",
        "                dill.dump(self.LABEL.vocab, f)\n",
        "\n",
        "\n",
        "        # generate batch iterator\n",
        "        self.train_iter, self.dev_iter, self.test_iter =  data.BucketIterator.splits((self.train, self.dev, self.test), batch_size=batch_size, device=gpu)\n",
        "\n",
        "    # check local train, dev, test sets\n",
        "    def if_splited(self):\n",
        "        for path in self.snli_split_path_lst:\n",
        "            if not os.path.exists(path):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # load dataset from local\n",
        "    def load_split_datasets(self, fields):\n",
        "        # load from local\n",
        "        with open('./data/snli_split/train', 'rb')as f:\n",
        "            train_examples = dill.load(f)\n",
        "        with open('./data/snli_split/dev', 'rb')as f:\n",
        "            dev_examples = dill.load(f)\n",
        "        with open('./data/snli_split/test', 'rb')as f:\n",
        "            test_examples = dill.load(f)\n",
        "\n",
        "        # recover\n",
        "        train = SNLIDataset(examples=train_examples, fields=fields)\n",
        "        dev = SNLIDataset(examples=dev_examples, fields=fields)\n",
        "        test = SNLIDataset(examples=test_examples, fields=fields)\n",
        "        return train, dev, test\n",
        "\n",
        "    # save datasets to local\n",
        "    def save_splited_sets(self, train, dev, test):\n",
        "        # save to local\n",
        "        with open('./data/snli_split/train', 'wb')as f:\n",
        "            dill.dump(train.examples, f)\n",
        "        with open('./data/snli_split/dev', 'wb')as f:\n",
        "            dill.dump(dev.examples, f)\n",
        "        with open('./data/snli_split/test', 'wb')as f:\n",
        "            dill.dump(test.examples, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGkJYKinOdrH",
        "colab_type": "text"
      },
      "source": [
        "## Initialize SNLI class and do preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C4zrNONWwSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')\n",
        "snli = SNLI(batch_size=32, gpu=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166rJ755O7jY",
        "colab_type": "text"
      },
      "source": [
        "## Bowman's Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UHTtZ3-b1QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bowman(nn.Module):\n",
        "    def __init__(self, vocab, premise_emb=300, hypothesis_emb=300, premise_d=100, hypothesis_d=100, lstm_layers=1, dropout=0.1):\n",
        "        super(Bowman, self).__init__()\n",
        "        # vocab - vocab built for corpus\n",
        "        # premise_emb - word embedding size for tokens in premise\n",
        "        # hypothesis_emb - word embedding size for tokens in hypothesis\n",
        "        # premise_d - sentence embedding size for premise\n",
        "        # hypothesis_d - sentence embedding size for hypothesis\n",
        "        # lstm_layers - layer number for LSTM model\n",
        "        # dropout - dropout rate for the model\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Premise_Enc = nn.LSTM(input_size=premise_emb, hidden_size=premise_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.Hypothesis_Enc = nn.LSTM(input_size=hypothesis_emb, hidden_size=hypothesis_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.out = nn.Linear(premise_d + hypothesis_d, 3) # batch_size x 3\n",
        "\n",
        "    def forward(self, premise_seq, hypothesis_seq):\n",
        "        premise_seq = self.embedding(premise_seq) # batch_size x seq_len -> batch_size x seq_len x 300\n",
        "        hypothesis_seq = self.embedding(hypothesis_seq) # batch_size x seq_len -> batch_size x seq_len x 300\n",
        "        premise_seq = self.dropout(premise_seq)\n",
        "        hypothesis_seq = self.dropout(hypothesis_seq)\n",
        "\n",
        "        premise_output, _  = self.Premise_Enc(premise_seq) # batch_size x seq_len x 300 -> batch_size x seq_len x 100\n",
        "        hypothesis_output, _  = self.Hypothesis_Enc(hypothesis_seq) # batch_size x seq_len x 300 -> batch_size x seq_len x 100\n",
        "        premise_output = torch.mean(premise_output, 1) # batch_size x seq_len x 100 -> batch_size x 100\n",
        "        hypothesis_output = torch.mean(hypothesis_output, 1) # batch_size x seq_len x 100 -> batch_size x 100\n",
        "        next_in = torch.cat((premise_output, hypothesis_output), 1)  # [batch_size x 100, batch_size x 100] -> batch_size x 200\n",
        "        #next_in = torch.cat((premise_output[ :, -1, :],hypothesis_output[ :, -1, :]), 1)\n",
        "        next_in = self.dropout(next_in)\n",
        "        tanh_out = self.tanh(self.tanh(self.tanh(next_in)))\n",
        "        output = self.out(tanh_out) # batch_size x 200 -> batch_size x 3\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NkEf568PJfH",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTYWSOXb6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_train(model, dataset, criterion, optimizer, epoch_num=5):\n",
        "    # model - model\n",
        "    # dataset - traning set\n",
        "    # criterion - loss function\n",
        "    # optimizer - optimize function\n",
        "    # epoch_num - epoch number\n",
        "    snli = dataset\n",
        "    # file to record average loss for each epoch\n",
        "    record = open(\"result.txt\", \"wb\", buffering=0)\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    for epoch in range(epoch_num):\n",
        "        running_loss = 0.0\n",
        "        epoch_loss = 0.0\n",
        "        i = 0\n",
        "        for batch in snli.train_iter:\n",
        "            i += 1\n",
        "            # get data\n",
        "            premise, _ = batch.premise\n",
        "            hypothesis, _ = batch.hypothesis\n",
        "            label = batch.label\n",
        "\n",
        "            # zeros the parameters gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize step\n",
        "            output = model(premise, hypothesis)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # add loss for the batch\n",
        "            running_loss += loss.item()\n",
        "            epoch_loss += loss.item()\n",
        "            if i % 1000 == 999:\n",
        "                # print average running loss for each 1000 batch\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch, i + 1, running_loss / 1000))\n",
        "                running_loss = 0.0\n",
        "        # print average loss for the epoch\n",
        "        print('epoch %d loss: %.3f\\n' % (epoch, epoch_loss / (i + 1)))\n",
        "        # save average loss for the epoch\n",
        "        record.write(b'%f\\n' % (epoch_loss / (i + 1)))\n",
        "        # save trained model after the epoch\n",
        "        torch.save(model, './model/bowman_%d.pth'% (epoch))\n",
        "\n",
        "    # save final trained model\n",
        "    torch.save(model, './model/bowman_final.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLBDP_gOPWzA",
        "colab_type": "text"
      },
      "source": [
        "Initial model, use cross entropy loss and Adam Delta SGD as optimize function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPnY8SIdxaC",
        "colab_type": "code",
        "outputId": "9346b7d1-34e7-4d05-d547-4ca2687e457b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "source": [
        "model = Bowman(snli.TEXT.vocab)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "bowman_train(model, snli, criterion, optimizer, epoch_num=1000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0,  1000] loss: 1.085\n",
            "[0,  2000] loss: 1.029\n",
            "[0,  3000] loss: 0.975\n",
            "[0,  4000] loss: 0.948\n",
            "[0,  5000] loss: 0.927\n",
            "[0,  6000] loss: 0.916\n",
            "[0,  7000] loss: 0.910\n",
            "[0,  8000] loss: 0.898\n",
            "[0,  9000] loss: 0.898\n",
            "[0, 10000] loss: 0.884\n",
            "[0, 11000] loss: 0.877\n",
            "[0, 12000] loss: 0.879\n",
            "[0, 13000] loss: 0.869\n",
            "[0, 14000] loss: 0.860\n",
            "[0, 15000] loss: 0.856\n",
            "[0, 16000] loss: 0.853\n",
            "[0, 17000] loss: 0.850\n",
            "epoch 0 loss: 0.912\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Bowman. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJMEXNWIPi62",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Do evaluation on both training set and test set, then print the predict accuracy on each set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFApqzysPDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_eval(model, dataset):\n",
        "    # model - model\n",
        "    # dataset - evaluation set\n",
        "    snli = dataset\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    c_count = 0.\n",
        "    t_count = 0.\n",
        "    for batch in snli.train_iter:\n",
        "        # get data\n",
        "        premise, _ = batch.premise\n",
        "        hypothesis, _ = batch.hypothesis\n",
        "        label = batch.label\n",
        "\n",
        "        # do predict\n",
        "        output = model(premise, hypothesis)\n",
        "        predict = torch.argmax(output, dim=1)\n",
        "\n",
        "        batch_size = predict.shape\n",
        "        # total number\n",
        "        t_count += batch_size[0]\n",
        "        # correct number\n",
        "        c_count += int(torch.sum(predict == label))\n",
        "    # calcualte the accuracy and print it out\n",
        "    print(\"Train acc.: %f\" % (c_count / t_count))\n",
        "    \n",
        "    c_count = 0.\n",
        "    t_count = 0.\n",
        "    for batch in snli.test_iter:\n",
        "        premise, _ = batch.premise\n",
        "        hypothesis, _ = batch.hypothesis\n",
        "        label = batch.label\n",
        "        output = model(premise, hypothesis)\n",
        "        predict = torch.argmax(output, dim=1)\n",
        "        batch_size = predict.shape\n",
        "        t_count += batch_size[0]\n",
        "        c_count += int(torch.sum(predict == label))\n",
        "    print(\"Test acc.: %f\" % (c_count / t_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u99QBJjIsSQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "a46e8957-8dbf-435e-ebc8-bfd296a6c0c8"
      },
      "source": [
        "#model = torch.load(\"./model/bowman_final.pt\")\n",
        "bowman_eval(model, snli)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train acc.: 0.619719\n",
            "Test acc.: 0.581230\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}