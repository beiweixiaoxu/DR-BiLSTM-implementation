{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bowman.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dKfdbsQDgJh",
        "colab_type": "code",
        "outputId": "19e0c04e-5499-47e6-914b-27765c432e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Dataset\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import time\n",
        "import dill\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU4j19JiV2kZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "2a6f3238-6954-4c32-b562-9e2f1cf31f4e"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "\n",
        "new_dir = []\n",
        "project_dir_path = './'\n",
        "data_dir_path = join(project_dir_path, 'data')\n",
        "model_dir_path = join(project_dir_path, 'model')\n",
        "new_dir.append(data_dir_path)\n",
        "new_dir.append(model_dir_path)\n",
        "snli_split_dir_path = join(data_dir_path, 'snli_split')\n",
        "snli_train_examples_path = join(snli_split_dir_path, 'train_examples')\n",
        "snli_dev_examples_path = join(snli_split_dir_path, 'dev_examples')\n",
        "snli_test_examples_path = join(snli_split_dir_path, 'test_examples')\n",
        "\n",
        "snli_split_path_lst = [snli_train_examples_path, snli_dev_examples_path, snli_test_examples_path]\n",
        "\n",
        "snli_text_vocab_path = join(snli_split_dir_path, 'text_vocab')\n",
        "snli_label_vocab_path = join(snli_split_dir_path, 'label_vocab')\n",
        "\n",
        "new_dir.append(snli_split_dir_path)\n",
        "\n",
        "for dir in new_dir:\n",
        "    if not os.path.exists(dir):\n",
        "        print('mkdir:', dir)\n",
        "        os.mkdir(dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: ./data\n",
            "mkdir: ./model\n",
            "mkdir: ./data/snli_split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T62lOGEWGhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLIDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return data.interleave_keys(\n",
        "            len(ex.premise), len(ex.hypothesis))\n",
        "\n",
        "\n",
        "class SNLI(object):\n",
        "    def __init__(self, batch_size=4, gpu=torch.device(torch.cuda.current_device())):\n",
        "        self.TEXT = data.Field(batch_first=True,\n",
        "                               include_lengths=True,\n",
        "                               tokenize=word_tokenize,\n",
        "                               lower=True)\n",
        "\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None)\n",
        "\n",
        "        # Split Dataset\n",
        "        if self.if_split_already():\n",
        "            print('Loading splited data set...')\n",
        "            fields = {'premise': self.TEXT, 'hypothesis': self.TEXT, 'label': self.LABEL}\n",
        "            self.train, self.dev, self.test = self.load_split_datasets(fields)\n",
        "        else:\n",
        "            print('No local data set detected, spliting...')\n",
        "            self.train, self.dev, self.test = datasets.SNLI.splits(self.TEXT, self.LABEL, root='data')\n",
        "            self.dump_examples(self.train, self.dev, self.test)\n",
        "\n",
        "\n",
        "        # Create Vocab\n",
        "        print('Building Vocab...')\n",
        "        # self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "        # self.LABEL.build_vocab(self.train)\n",
        "        if os.path.exists(snli_text_vocab_path) and os.path.exists(snli_label_vocab_path):\n",
        "            print('Loading local Vocab...')\n",
        "            with open(snli_text_vocab_path, 'rb')as f:\n",
        "                self.TEXT.vocab = dill.load(f)\n",
        "            with open(snli_label_vocab_path, 'rb')as f:\n",
        "                self.LABEL.vocab = dill.load(f)\n",
        "        else:\n",
        "            print('No local Vocab detected, building...')\n",
        "            self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "            self.LABEL.build_vocab(self.train)\n",
        "            with open(snli_text_vocab_path, 'wb')as f:\n",
        "                dill.dump(self.TEXT.vocab, f)\n",
        "            with open(snli_label_vocab_path, 'wb')as f:\n",
        "                dill.dump(self.LABEL.vocab, f)\n",
        "\n",
        "\n",
        "        # Generate batch iterator\n",
        "        print('Generating batch iter...')\n",
        "        self.train_iter, self.dev_iter, self.test_iter = \\\n",
        "            data.BucketIterator.splits((self.train, self.dev, self.test),\n",
        "                                       batch_size=batch_size,\n",
        "                                       device=gpu)\n",
        "\n",
        "    def if_split_already(self):\n",
        "        for path in snli_split_path_lst:\n",
        "            if not os.path.exists(path):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # Load dataset from local\n",
        "    def load_split_datasets(self, fields):\n",
        "        # Loading examples\n",
        "        with open(snli_train_examples_path, 'rb')as f:\n",
        "            train_examples = dill.load(f)\n",
        "        with open(snli_dev_examples_path, 'rb')as f:\n",
        "            dev_examples = dill.load(f)\n",
        "        with open(snli_test_examples_path, 'rb')as f:\n",
        "            test_examples = dill.load(f)\n",
        "\n",
        "        # Recover dataset\n",
        "        train = SNLIDataset(examples=train_examples, fields=fields)\n",
        "        dev = SNLIDataset(examples=dev_examples, fields=fields)\n",
        "        test = SNLIDataset(examples=test_examples, fields=fields)\n",
        "        return train, dev, test\n",
        "\n",
        "    # Save to local\n",
        "    def dump_examples(self, train, dev, test):\n",
        "        # Save examples\n",
        "        if not os.path.exists(snli_train_examples_path):\n",
        "            with open(snli_train_examples_path, 'wb')as f:\n",
        "                dill.dump(train.examples, f)\n",
        "        if not os.path.exists(snli_dev_examples_path):\n",
        "            with open(snli_dev_examples_path, 'wb')as f:\n",
        "                dill.dump(dev.examples, f)\n",
        "        if not os.path.exists(snli_test_examples_path):\n",
        "            with open(snli_test_examples_path, 'wb')as f:\n",
        "                dill.dump(test.examples, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C4zrNONWwSi",
        "colab_type": "code",
        "outputId": "533fd5ff-90ed-42ac-89f9-77bffafb8996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "snli = SNLI(batch_size=32, gpu=device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No local data set detected, spliting...\n",
            "downloading snli_1.0.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "snli_1.0.zip: 100%|██████████| 94.6M/94.6M [00:04<00:00, 20.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r.vector_cache/glove.840B.300d.zip: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building Vocab...\n",
            "No local Vocab detected, building...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.840B.300d.zip: 2.18GB [16:54, 2.15MB/s]                            \n",
            "100%|█████████▉| 2194853/2196017 [03:20<00:00, 11962.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating batch iter...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UHTtZ3-b1QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bowman(nn.Module):\n",
        "    def __init__(self, vocab, premise_emb=300, hypothesis_emb=300, premise_d=100, hypothesis_d=100, lstm_layers=1, dropout=0.1):\n",
        "        super(Bowman, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Premise_Enc = nn.LSTM(input_size=premise_emb, hidden_size=premise_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.Hypothesis_Enc = nn.LSTM(input_size=hypothesis_emb, hidden_size=hypothesis_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.out = nn.Linear(premise_d + hypothesis_d, 3)\n",
        "\n",
        "    def forward(self, premise_seq, hypothesis_seq):\n",
        "        premise_seq = self.embedding(premise_seq)\n",
        "        hypothesis_seq = self.embedding(hypothesis_seq)\n",
        "        premise_seq = self.dropout(premise_seq)\n",
        "        hypothesis_seq = self.dropout(hypothesis_seq)\n",
        "\n",
        "        premise_output, _  = self.Premise_Enc(premise_seq)\n",
        "        hypothesis_output, _  = self.Hypothesis_Enc(hypothesis_seq)\n",
        "        premise_output = torch.mean(premise_output, 1)\n",
        "        hypothesis_output = torch.mean(hypothesis_output, 1)\n",
        "        next_in = torch.cat((premise_output, hypothesis_output), 1)\n",
        "        #next_in = torch.cat((premise_output[ :, -1, :],hypothesis_output[ :, -1, :]), 1)\n",
        "        next_in = self.dropout(next_in)\n",
        "        tanh_out = self.tanh(self.tanh(self.tanh(next_in)))\n",
        "        output = self.out(tanh_out)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTYWSOXb6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_train(model, dataset, criterion, optimizer, epoch_num=5):\n",
        "    snli = dataset\n",
        "    record = open(\"result.txt\", \"wb\", buffering=0)\n",
        "    model.train()\n",
        "    for epoch in range(epoch_num):\n",
        "\n",
        "        running_loss = 0.0\n",
        "        epoch_loss = 0.0\n",
        "        i = 0\n",
        "        for batch in snli.train_iter:\n",
        "            i += 1\n",
        "            #get data\n",
        "        \n",
        "            premise, _ = batch.premise\n",
        "            hypothesis, _ = batch.hypothesis\n",
        "            label = batch.label\n",
        "\n",
        "            # zeros the paramster gradients\n",
        "            optimizer.zero_grad()       # \n",
        "\n",
        "            # forward + backward + optimize\n",
        "            output = model(premise, hypothesis)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_loss += loss.item()\n",
        "            if i % 1000 == 999:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch, i + 1, running_loss / 1000))\n",
        "                running_loss = 0.0\n",
        "        print('epoch %d loss: %.3f\\n' % (epoch, epoch_loss / (i + 1)))\n",
        "        record.write(b'%f\\n' % (epoch_loss / (i + 1)))\n",
        "        torch.save(model, './model/bowman_%d.pth'% (epoch))\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(model, './model/bowman_final.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPnY8SIdxaC",
        "colab_type": "code",
        "outputId": "ca947393-fbff-4f01-dc69-555e7e7dc881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "model = Bowman(snli.TEXT.vocab)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n",
        "\n",
        "bowman_train(model, snli, criterion, optimizer, epoch_num=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0,  1000] loss: 1.084\n",
            "[0,  2000] loss: 1.032\n",
            "[0,  3000] loss: 0.979\n",
            "[0,  4000] loss: 0.946\n",
            "[0,  5000] loss: 0.930\n",
            "[0,  6000] loss: 0.919\n",
            "[0,  7000] loss: 0.906\n",
            "[0,  8000] loss: 0.899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFApqzysPDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_eval(model, dataset):\n",
        "    snli = dataset\n",
        "    model.eval()\n",
        "\n",
        "    c_count = 0.\n",
        "    t_count = 0.\n",
        "    for batch in snli.train_iter:\n",
        "        premise, _ = batch.premise\n",
        "        hypothesis, _ = batch.hypothesis\n",
        "        label = batch.label\n",
        "        output = model(premise, hypothesis)\n",
        "        predict = torch.argmax(output, dim=1)\n",
        "        batch_size = predict.shape\n",
        "        t_count += batch_size[0]\n",
        "        c_count += int(torch.sum(predict == label))\n",
        "    print(\"Train acc.: %f\" % (c_count / t_count))\n",
        "    \n",
        "    c_count = 0.\n",
        "    t_count = 0.\n",
        "    for batch in snli.test_iter:\n",
        "        premise, _ = batch.premise\n",
        "        hypothesis, _ = batch.hypothesis\n",
        "        label = batch.label\n",
        "        output = model(premise, hypothesis)\n",
        "        predict = torch.argmax(output, dim=1)\n",
        "        batch_size = predict.shape\n",
        "        t_count += batch_size[0]\n",
        "        c_count += int(torch.sum(predict == label))\n",
        "    print(\"Test acc.: %f\" % (c_count / t_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u99QBJjIsSQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = torch.load(\"./model/bowman_final.pt\")\n",
        "bowman_eval(model, snli)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}                    8000] loss: 0.793\n",
            "[2,  9000] loss: 0.797\n",
            "[2, 10000] loss: 0.792\n",
            "[2, 11000] loss: 0.795\n",
            "[2, 12000] loss: 0.787\n",
            "[2, 13000] loss: 0.791\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}