{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bowman.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhBAy7RsMdDc",
        "colab_type": "text"
      },
      "source": [
        "# Bowman's Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut10idNFMQ71",
        "colab_type": "text"
      },
      "source": [
        "## Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dKfdbsQDgJh",
        "colab_type": "code",
        "outputId": "bdd7400b-fa0f-4364-fd1f-772fd194d414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Dataset\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import time\n",
        "import dill\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUcE6lFVMsnW",
        "colab_type": "text"
      },
      "source": [
        "## Make directories\n",
        "Make directories for saving data and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU4j19JiV2kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "\n",
        "# claim directories for saving data and model\n",
        "new_dir = ['./data', './model', './data/snli_split']\n",
        "\n",
        "# if directories not exist, make new directory\n",
        "for dir in new_dir:\n",
        "    if not os.path.exists(dir):\n",
        "        print('mkdir:', dir)\n",
        "        os.mkdir(dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY_HL4ngM839",
        "colab_type": "text"
      },
      "source": [
        "## SNLI corpus data preprocessing\n",
        "The class to preprocess SNLI corpus.\n",
        "\n",
        "For the first time on initial, the initial function will download and split SNLI corpus to train, dev and test sets, build vocab for the corpus by using Glove 840B 300d and store them to local files. And it will also generate batch iterator for the sets. This process may take more than 5 minutes on first time.\n",
        "\n",
        "After the first time, initial function will check existence of local files, if exists, it will load directly from local file and thus can save a lot of time (within 1 minute)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T62lOGEWGhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLIDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return data.interleave_keys(len(ex.premise), len(ex.hypothesis))\n",
        "\n",
        "# preprocess SNLI corpus to save time and give train, dev, test sets\n",
        "class SNLI(object):\n",
        "    def __init__(self, batch_size=4, gpu=torch.device('cuda')):\n",
        "        # set file name for train dev test sets\n",
        "        self.snli_split_path_lst = ['./data/snli_split/train', './data/snli_split/dev', './data/snli_split/test']\n",
        "\n",
        "        # set data field for text and label\n",
        "        self.TEXT = data.Field(batch_first=True, include_lengths=True, tokenize=word_tokenize, lower=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None)\n",
        "\n",
        "        # split corpus\n",
        "        if self.if_splited():\n",
        "            # if already splited, load local sets\n",
        "            fields = {'premise': self.TEXT, 'hypothesis': self.TEXT, 'label': self.LABEL}\n",
        "            self.train, self.dev, self.test = self.load_split_datasets(fields)\n",
        "        else:\n",
        "            # split corpus to train, dev, test sets and save them to local\n",
        "            self.train, self.dev, self.test = datasets.SNLI.splits(self.TEXT, self.LABEL, root='data')\n",
        "            self.save_splited_sets(self.train, self.dev, self.test)\n",
        "\n",
        "\n",
        "        # build vocab for corpus\n",
        "        if os.path.exists('./data/snli_split/text_vocab') and os.path.exists('./data/snli_split/label_vocab'):\n",
        "            # if local vocab exists, load local vocab into model\n",
        "            with open('./data/snli_split/text_vocab', 'rb')as f:\n",
        "                self.TEXT.vocab = dill.load(f)\n",
        "            with open('./data/snli_split/label_vocab', 'rb')as f:\n",
        "                self.LABEL.vocab = dill.load(f)\n",
        "        else:\n",
        "            # build vocab for corpus and save it to local\n",
        "            self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "            self.LABEL.build_vocab(self.train)\n",
        "            with open('./data/snli_split/text_vocab', 'wb')as f:\n",
        "                dill.dump(self.TEXT.vocab, f)\n",
        "            with open('./data/snli_split/label_vocab', 'wb')as f:\n",
        "                dill.dump(self.LABEL.vocab, f)\n",
        "\n",
        "\n",
        "        # generate batch iterator\n",
        "        self.train_iter, self.dev_iter, self.test_iter =  data.BucketIterator.splits((self.train, self.dev, self.test), batch_size=batch_size, device=gpu)\n",
        "\n",
        "    # check local train, dev, test sets\n",
        "    def if_splited(self):\n",
        "        for path in self.snli_split_path_lst:\n",
        "            if not os.path.exists(path):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # load dataset from local\n",
        "    def load_split_datasets(self, fields):\n",
        "        # load from local\n",
        "        with open('./data/snli_split/train', 'rb')as f:\n",
        "            train_examples = dill.load(f)\n",
        "        with open('./data/snli_split/dev', 'rb')as f:\n",
        "            dev_examples = dill.load(f)\n",
        "        with open('./data/snli_split/test', 'rb')as f:\n",
        "            test_examples = dill.load(f)\n",
        "\n",
        "        # recover\n",
        "        train = SNLIDataset(examples=train_examples, fields=fields)\n",
        "        dev = SNLIDataset(examples=dev_examples, fields=fields)\n",
        "        test = SNLIDataset(examples=test_examples, fields=fields)\n",
        "        return train, dev, test\n",
        "\n",
        "    # save datasets to local\n",
        "    def save_splited_sets(self, train, dev, test):\n",
        "        # save to local\n",
        "        with open('./data/snli_split/train', 'wb')as f:\n",
        "            dill.dump(train.examples, f)\n",
        "        with open('./data/snli_split/dev', 'wb')as f:\n",
        "            dill.dump(dev.examples, f)\n",
        "        with open('./data/snli_split/test', 'wb')as f:\n",
        "            dill.dump(test.examples, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGkJYKinOdrH",
        "colab_type": "text"
      },
      "source": [
        "## Initialize SNLI class and do preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C4zrNONWwSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')\n",
        "snli = SNLI(batch_size=32, gpu=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166rJ755O7jY",
        "colab_type": "text"
      },
      "source": [
        "## Bowman's Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UHTtZ3-b1QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bowman(nn.Module):\n",
        "    def __init__(self, vocab, premise_emb=300, hypothesis_emb=300, premise_d=100, hypothesis_d=100, lstm_layers=1, dropout=0.1):\n",
        "        super(Bowman, self).__init__()\n",
        "        # vocab - vocab built for corpus\n",
        "        # premise_emb - word embedding size for tokens in premise\n",
        "        # hypothesis_emb - word embedding size for tokens in hypothesis\n",
        "        # premise_d - sentence embedding size for premise\n",
        "        # hypothesis_d - sentence embedding size for hypothesis\n",
        "        # lstm_layers - layer number for LSTM model\n",
        "        # dropout - dropout rate for the model\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Premise_Enc = nn.LSTM(input_size=premise_emb, hidden_size=premise_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.Hypothesis_Enc = nn.LSTM(input_size=hypothesis_emb, hidden_size=hypothesis_d, num_layers=lstm_layers, batch_first=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.out = nn.Linear(premise_d + hypothesis_d, 3) # batch_size x 3\n",
        "\n",
        "    def forward(self, premise_seq, hypothesis_seq):\n",
        "        premise_seq = self.embedding(premise_seq) # batch_size x seq_len -> batch_size x seq_len x 300\n",
        "        hypothesis_seq = self.embedding(hypothesis_seq) # batch_size x seq_len -> batch_size x seq_len x 300\n",
        "        premise_seq = self.dropout(premise_seq)\n",
        "        hypothesis_seq = self.dropout(hypothesis_seq)\n",
        "\n",
        "        premise_output, _  = self.Premise_Enc(premise_seq) # batch_size x seq_len x 300 -> batch_size x seq_len x 100\n",
        "        hypothesis_output, _  = self.Hypothesis_Enc(hypothesis_seq) # batch_size x seq_len x 300 -> batch_size x seq_len x 100\n",
        "        premise_output = torch.mean(premise_output, 1) # batch_size x seq_len x 100 -> batch_size x 100\n",
        "        hypothesis_output = torch.mean(hypothesis_output, 1) # batch_size x seq_len x 100 -> batch_size x 100\n",
        "        next_in = torch.cat((premise_output, hypothesis_output), 1)  # [batch_size x 100, batch_size x 100] -> batch_size x 200\n",
        "        #next_in = torch.cat((premise_output[ :, -1, :],hypothesis_output[ :, -1, :]), 1)\n",
        "        next_in = self.dropout(next_in)\n",
        "        tanh_out = self.tanh(self.tanh(self.tanh(next_in)))\n",
        "        output = self.out(tanh_out) # batch_size x 200 -> batch_size x 3\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NkEf568PJfH",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTYWSOXb6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_train(model, dataset, criterion, optimizer, epoch_num=5):\n",
        "    # model - model\n",
        "    # dataset - traning set\n",
        "    # criterion - loss function\n",
        "    # optimizer - optimize function\n",
        "    # epoch_num - epoch number\n",
        "    snli = dataset\n",
        "    # file to record average loss for each epoch\n",
        "    record = open(\"result.txt\", \"wb\", buffering=0)\n",
        "    for epoch in range(epoch_num):\n",
        "        # switch to train mode\n",
        "        model.train()\n",
        "\n",
        "        for batch in snli.train_iter:\n",
        "            # get data\n",
        "            premise, _ = batch.premise\n",
        "            hypothesis, _ = batch.hypothesis\n",
        "            label = batch.label\n",
        "\n",
        "            # zeros the parameters gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize step\n",
        "            output = model(premise, hypothesis)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc, train_loss = bowman_eval(model, dataset, \"Train\", criterion)\n",
        "        dev_acc, dev_loss = bowman_eval(model, dataset, \"Dev\", criterion)\n",
        "        # print average loss for the epoch\n",
        "        print('epoch %d train_loss: %.3f dev_loss: %.3f train_acc: %.3f dev_acc: %.3f' % (epoch, train_loss, dev_loss, train_acc, dev_acc))\n",
        "        # save average loss for the epoch\n",
        "        record.write(b'%f\\t%f\\t%f\\t%f\\n' % (train_loss, dev_loss, train_acc, dev_acc))\n",
        "        # save trained model after the epoch\n",
        "        torch.save(model.state_dict(), './model/bowman_%d.pth'% (epoch))\n",
        "\n",
        "    # save final trained model\n",
        "    torch.save(model.state_dict(), './model/bowman_final.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLBDP_gOPWzA",
        "colab_type": "text"
      },
      "source": [
        "Initial model, use cross entropy loss and Adam Delta SGD as optimize function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPnY8SIdxaC",
        "colab_type": "code",
        "outputId": "82fda630-c23d-4981-9923-26b22cf05ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "model = Bowman(snli.TEXT.vocab)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "bowman_train(model, snli, criterion, optimizer, epoch_num=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 train_loss: 1.015 dev_loss: 1.030 train_acc: 0.494 dev_acc: 0.476\n",
            "epoch 1 train_loss: 0.923 dev_loss: 0.946 train_acc: 0.570 dev_acc: 0.569\n",
            "epoch 2 train_loss: 0.889 dev_loss: 0.929 train_acc: 0.594 dev_acc: 0.587\n",
            "epoch 3 train_loss: 0.872 dev_loss: 0.917 train_acc: 0.604 dev_acc: 0.591\n",
            "epoch 4 train_loss: 0.854 dev_loss: 0.902 train_acc: 0.615 dev_acc: 0.600\n",
            "epoch 5 train_loss: 0.842 dev_loss: 0.894 train_acc: 0.621 dev_acc: 0.599\n",
            "epoch 6 train_loss: 0.825 dev_loss: 0.894 train_acc: 0.631 dev_acc: 0.593\n",
            "epoch 7 train_loss: 0.815 dev_loss: 0.890 train_acc: 0.637 dev_acc: 0.594\n",
            "epoch 8 train_loss: 0.810 dev_loss: 0.893 train_acc: 0.640 dev_acc: 0.585\n",
            "epoch 9 train_loss: 0.800 dev_loss: 0.881 train_acc: 0.646 dev_acc: 0.597\n",
            "epoch 10 train_loss: 0.794 dev_loss: 0.874 train_acc: 0.649 dev_acc: 0.602\n",
            "epoch 11 train_loss: 0.799 dev_loss: 0.875 train_acc: 0.647 dev_acc: 0.604\n",
            "epoch 12 train_loss: 0.783 dev_loss: 0.864 train_acc: 0.655 dev_acc: 0.610\n",
            "epoch 13 train_loss: 0.781 dev_loss: 0.862 train_acc: 0.657 dev_acc: 0.611\n",
            "epoch 14 train_loss: 0.776 dev_loss: 0.865 train_acc: 0.659 dev_acc: 0.604\n",
            "epoch 15 train_loss: 0.783 dev_loss: 0.854 train_acc: 0.657 dev_acc: 0.617\n",
            "epoch 16 train_loss: 0.768 dev_loss: 0.860 train_acc: 0.664 dev_acc: 0.608\n",
            "epoch 17 train_loss: 0.765 dev_loss: 0.861 train_acc: 0.666 dev_acc: 0.605\n",
            "epoch 18 train_loss: 0.761 dev_loss: 0.859 train_acc: 0.667 dev_acc: 0.607\n",
            "epoch 19 train_loss: 0.761 dev_loss: 0.853 train_acc: 0.667 dev_acc: 0.614\n",
            "epoch 20 train_loss: 0.761 dev_loss: 0.866 train_acc: 0.666 dev_acc: 0.592\n",
            "epoch 21 train_loss: 0.761 dev_loss: 0.850 train_acc: 0.669 dev_acc: 0.617\n",
            "epoch 22 train_loss: 0.753 dev_loss: 0.852 train_acc: 0.672 dev_acc: 0.613\n",
            "epoch 23 train_loss: 0.750 dev_loss: 0.847 train_acc: 0.674 dev_acc: 0.618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJMEXNWIPi62",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Do evaluation on both training set and test set, then print the predict accuracy on each set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFApqzysPDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bowman_eval(model, dataset, set_name, criterion):\n",
        "    # model - model\n",
        "    # dataset - evaluation set\n",
        "    snli = dataset\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    batch_iter = None\n",
        "\n",
        "    if set_name == \"Train\":\n",
        "        batch_iter = snli.train_iter\n",
        "    elif set_name == \"Dev\":\n",
        "        batch_iter = snli.dev_iter\n",
        "    elif set_name == \"Test\":\n",
        "        batch_iter = snli.test_iter\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    c_count = 0.\n",
        "    t_count = 0.\n",
        "    epoch_loss = 0.0\n",
        "    for batch in batch_iter:\n",
        "        # get data\n",
        "        premise, _ = batch.premise\n",
        "        hypothesis, _ = batch.hypothesis\n",
        "        label = batch.label\n",
        "\n",
        "        # do predict\n",
        "        output = model(premise, hypothesis)\n",
        "        predict = torch.argmax(output, dim=1)\n",
        "        loss = criterion(output, label)\n",
        "        batch_size = predict.shape\n",
        "\n",
        "        epoch_loss += loss.item() * batch_size[0]\n",
        "\n",
        "        # total number\n",
        "        t_count += batch_size[0]\n",
        "        # correct number\n",
        "        c_count += int(torch.sum(predict == label))\n",
        "    # calcualte the accuracy and print it out\n",
        "    # print(\"%s acc.: %f\" % (set_name, c_count / t_count))\n",
        "    return c_count / t_count, epoch_loss / t_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u99QBJjIsSQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "9fb30fe2-73a7-429f-9ba3-7bd4e1ed69e7"
      },
      "source": [
        "#model = Bowman()\n",
        "#model.load_state_dict(torch.load(\"./model/bowman_final.pth\"))\n",
        "acc, loss = bowman_eval(model, snli, \"Train\", criterion)\n",
        "print(\"Train acc.: %.3f, loss : %.3f\" % (acc, loss))\n",
        "acc, loss = bowman_eval(model, snli, \"Test\", criterion)\n",
        "print(\"Test acc.: %.3f, loss : %.3f\" % (acc, loss))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train acc.: 0.705, loss : 0.695\n",
            "Test acc.: 0.664, loss : 0.796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUbkp3C7QUDT",
        "colab_type": "text"
      },
      "source": [
        "Result for first 5 sentences in test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U38_LPDOQNpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#device = torch.device('cuda')\n",
        "#model = torch.load(\"./model/bowman_11.pt\")\n",
        "#model.to(device)\n",
        "\n",
        "# first 5 premises with hypothesis\n",
        "premises = [\"This church choir sings to the masses as they sing joyous songs from the book at a church.\",\n",
        "\"This church choir sings to the masses as they sing joyous songs from the book at a church.\",\n",
        "\"This church choir sings to the masses as they sing joyous songs from the book at a church.\",\n",
        "\"A woman with a green headscarf, blue shirt and a very big grin.\",\n",
        "\"A woman with a green headscarf, blue shirt and a very big grin.\"]\n",
        "\n",
        "hypothesis = [\"The church has cracks in the ceiling.\",\n",
        "\"The church is filled with song.\",\n",
        "\"A choir singing at a baseball game.\",\n",
        "\"The woman is young.\",\n",
        "\"The woman is very happy.\"]\n",
        "\n",
        "# ground truth\n",
        "gold_label = [\"neural\", \"entailment\", \"contradiction\", \"neural\", \"entailment\"]\n",
        "\n",
        "# tokenize\n",
        "premises_token = [snli.TEXT.preprocess(x) for x in premises]\n",
        "hypothesis_token = [snli.TEXT.preprocess(x) for x in hypothesis]\n",
        "\n",
        "# label list\n",
        "label_vocab = snli.LABEL.vocab.itos\n",
        "preds = []\n",
        "\n",
        "for i in range(len(premises)):\n",
        "    # token to index in vocab\n",
        "    prem, _ = snli.TEXT.numericalize(([premises_token[i]],[len(premises_token[i])]), device=device)\n",
        "    hypo, _ = snli.TEXT.numericalize(([hypothesis_token[i]],[len(hypothesis_token[i])]), device=device)\n",
        "    # do prediction\n",
        "    output = model(prem, hypo)\n",
        "    lab = label_vocab[int(torch.argmax(output))]\n",
        "    preds.append(lab)\n",
        "\n",
        "# print results\n",
        "for i in range(len(premises)):\n",
        "    print(\"Premise: \" + premises[i])\n",
        "    print(\"Hypothesis: \" + hypothesis[i])\n",
        "    print(\"Model Output: \" + preds[i])\n",
        "    print(\"Ground Truth: \" + gold_label[i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}